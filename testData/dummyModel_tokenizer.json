{"class_name": "Tokenizer", "config": {"num_words": null, "filters": "!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n", "lower": true, "split": " ", "char_level": false, "oov_token": null, "document_count": 5, "word_counts": "{\"hello\": 1, \"hi\": 1, \"there\": 1, \"what\": 1, \"meow\": 1}", "word_docs": "{\"hello\": 1, \"hi\": 1, \"there\": 1, \"what\": 1, \"meow\": 1}", "index_docs": "{\"1\": 1, \"2\": 1, \"3\": 1, \"4\": 1, \"5\": 1}", "index_word": "{\"1\": \"hello\", \"2\": \"hi\", \"3\": \"there\", \"4\": \"what\", \"5\": \"meow\"}", "word_index": "{\"hello\": 1, \"hi\": 2, \"there\": 3, \"what\": 4, \"meow\": 5}"}}